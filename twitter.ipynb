{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "This deals with exclusively with the data acquistion, munging, and cleaning of the twitter data about Donald Trump from November 28th to 30th, 2015. If you want to see the final product, you can go [here](twitter_plot.html).\n",
    "\n",
    "**Grabbing Tweets**\n",
    "\n",
    "I used the Twitter API to grab tweets from November 29th to November 30th, the code can be seen in `streaming.py`. I essentailly left it streaming overnight to collect enough tweets for analysis. After that, I created a list and read the `.txt` files (there were interruptions so I had to separate the tweets) and append them on according. All in all, I got around 285,000 tweets. Good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from dateutil import parser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unirest\n",
    "\n",
    "import plotly.tools as tls\n",
    "import plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets_data = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    data = 'data/data' + str(i) + '.txt'\n",
    "    tweets_file = open(data, \"r\")\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "print len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get raw tweet text from twitter\n",
    "data = []\n",
    "for item in tweets_data:\n",
    "    data.append(item.get(\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Cleaning the Data**\n",
    "\n",
    "The next step is to take the tweets and cleaned them up. I borrowed [code](http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/#motivation) that creates a function that will strip and removed instances of retweets, mentions of other twitter users, and various keys. Then I used regular expression again, to remove `rt`, `AT_USER`, and `URL` which are the by-products of the `processTweet` function. \n",
    "\n",
    "The reason I kept it in two stages is because I wanted to make sure the function works correctly. I also stripped the times out of the tweets so I can plot for later uses.\n",
    "\n",
    "Finally, I combined the tweets and the times into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#processed the tweets to remove extra characters\n",
    "processed = []\n",
    "for tweet in data:\n",
    "    try:\n",
    "        processed.append(processing.processTweet(tweet))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "#format the tweet to remove AT_USER and URL and format from unicode text\n",
    "formatted = [re.sub(\"^rt|AT_USER|URL\", \"\", unicodedata.normalize('NFKD', item).encode('ascii','ignore')) for item in processed]\n",
    "\n",
    "#extract time of tweets\n",
    "times = []\n",
    "for item in tweets_data:\n",
    "    times.append(item.get(\"created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#convert the lists to a dataframe\n",
    "data = pd.DataFrame([times, formatted])\n",
    "data = data.transpose()\n",
    "data.columns = ['times','tweet']\n",
    "data['tweet'] = map(lambda x: str(x).strip(), data['tweet'])\n",
    "data.to_csv('tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "I used another [API](https://market.mashape.com/twinword/sentiment-analysis-free) that provides sentiment analysis for free. After extracting the tweets, I ran a loop that will allow the API to score each tweet as either `negative`, `neutral`, and `positive`\n",
    "\n",
    "I know that sentiment analysis has its flaws, such as unable to detect sarcasm or recognize the tone of certain words (for instance, \"not bad\" can be construed as negative, even though it could be actually positive). However this is good enough to get a good idea of the sentiments of our tweets (the idea, after all, is to build something).\n",
    "\n",
    "After getting the sentiments of all the tweets, I joined the list with my data frame and saved it since it took a day or so! (ran into some more hiccups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a list to store sentiment of each text, looping through the dataframe, and appending it on.\n",
    "sentiment = []\n",
    "for text in data['tweet']:\n",
    "    response = unirest.post(\"https://twinword-sentiment-analysis.p.mashape.com/analyze/\",\n",
    "      headers={\n",
    "        \"X-Mashape-Key\": os.environ.get('NLP_KEY'),\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"Accept\": \"application/json\"\n",
    "      },\n",
    "      params={\n",
    "        \"text\": text\n",
    "      }\n",
    "    )\n",
    "    sentiment.append(response.body.items()[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Nov 28 18:46:24 +0000 2015</td>\n",
       "      <td>trump2016 they love donald!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Nov 28 18:46:25 +0000 2015</td>\n",
       "      <td>trump: we have to accept migrants here because...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat Nov 28 18:46:25 +0000 2015</td>\n",
       "      <td>media accuse trump of mocking disabled reporte...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Nov 28 18:46:25 +0000 2015</td>\n",
       "      <td>how donald trump comes up with his ideas</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat Nov 28 18:46:26 +0000 2015</td>\n",
       "      <td>me: i hate stamps donald trump: i hate stamps me:</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            times  \\\n",
       "0  Sat Nov 28 18:46:24 +0000 2015   \n",
       "1  Sat Nov 28 18:46:25 +0000 2015   \n",
       "2  Sat Nov 28 18:46:25 +0000 2015   \n",
       "3  Sat Nov 28 18:46:25 +0000 2015   \n",
       "4  Sat Nov 28 18:46:26 +0000 2015   \n",
       "\n",
       "                                               tweet sentiment  \n",
       "0                        trump2016 they love donald!  positive  \n",
       "1  trump: we have to accept migrants here because...  negative  \n",
       "2  media accuse trump of mocking disabled reporte...  negative  \n",
       "3           how donald trump comes up with his ideas   neutral  \n",
       "4  me: i hate stamps donald trump: i hate stamps me:  negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some cleaning up of the final dataframe\n",
    "withSentiment = data.join(pd.DataFrame(sentiment))\n",
    "withSentiment = withSentiment.iloc[:,[0,1,3]]\n",
    "withSentiment.columns = ['times','tweet','sentiment']\n",
    "withSentiment.head()\n",
    "withSentiment.to_csv('withSentiment.csv', index = False)\n",
    "\n",
    "#sneak peak at the resulting data.\n",
    "withSentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping the Data**\n",
    "\n",
    "Now that I have the data, I have to reshape it in a way that's meaningful for plotting. It doesn't help that the tweets are classified, what I'm interested in is how many classification did each time period had. To do this, I invoked the `pivot_table` method and reshaped it to count the number of classifications for each second. After doing that, I used a function from the `parser` library to convert the string text to the proper `datetime` format (converting \"Sat Nov 28 18:46:24 +0000 2015\" to \"2015-11-28 18:45:00+00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pivot table to aggregate tweet sentiment\n",
    "pivotedTable = withSentiment.pivot_table(index = 'times',  columns='sentiment', aggfunc=len)\n",
    "pivotedTable = pivotedTable.fillna(0)\n",
    "data = pivotedTable['tweet'] #extracting part of the dataframe so I don't have to deal with multi-level indexing as much\n",
    "\n",
    "#create a custom function to parse times\n",
    "def parseTimes(time):\n",
    "    try:\n",
    "        return parser.parse(time)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "#list comprehension to parse times\n",
    "listOfTimes = [parseTimes(time) for time in data.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "After converting the data to the proper format, I realized that the time interval is too granular, so I used the `resample` method to aggregate the data into 5 minute interval. I figured this is a good trade-off between granularity and cleanliness. I also removed any 5 minute periods that did not have any tweets as to not skew the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-11-28 18:45:00+00:00</th>\n",
       "      <td>499</td>\n",
       "      <td>191</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-28 18:50:00+00:00</th>\n",
       "      <td>700</td>\n",
       "      <td>372</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-28 18:55:00+00:00</th>\n",
       "      <td>722</td>\n",
       "      <td>352</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-28 19:00:00+00:00</th>\n",
       "      <td>641</td>\n",
       "      <td>335</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-28 19:05:00+00:00</th>\n",
       "      <td>637</td>\n",
       "      <td>323</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment                  negative  neutral  positive\n",
       "2015-11-28 18:45:00+00:00       499      191       373\n",
       "2015-11-28 18:50:00+00:00       700      372       508\n",
       "2015-11-28 18:55:00+00:00       722      352       660\n",
       "2015-11-28 19:00:00+00:00       641      335       520\n",
       "2015-11-28 19:05:00+00:00       637      323       498"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing the index of dataframe to match datetime class\n",
    "data = data.set_index(pd.DatetimeIndex(listOfTimes))\n",
    "\n",
    "#using resampling method to aggregate tweets again, in 5 minute increment this time.\n",
    "data = data.resample('5Min', how = 'sum')\n",
    "data.head()\n",
    "\n",
    "#remove period of times where there are no tweets at all\n",
    "data['sum'] = sum([data['negative'],data['positive'],data['neutral']])\n",
    "data = data.ix[data['sum'] > 0, ['negative','neutral','positive']]\n",
    "data.index = [dates.replace(\":00+00:00\",\"\").replace(\"2015-11\",\"Nov\") for dates in data.index]\n",
    "data.to_csv('finaldata.csv')\n",
    "\n",
    "#quick look at our cleaned dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**\n",
    "\n",
    "After acquiring, munging, and cleaning the data, we can plot to see the results. Plotting is done using `Plot.ly` which is super nifty. Since Github doesn't allow large files, I had to embed the code. Below you will see the code that I used to create the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code to create the plot\n",
    "\n",
    "#required to plot offline \n",
    "init_notebook_mode()\n",
    "\n",
    "#loading in file\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/minh5/nlp-sentiment/master/csv/finaldata.csv', index_col=0)\n",
    "\n",
    "# #plotting\n",
    "iplot({\n",
    "    'data': [\n",
    "        Scatter(x=data.index,\n",
    "                y=data[col],\n",
    "                name=col) for col in data.columns],\n",
    "    'layout': Layout(title=('Sentiment Analysis of Donald Trumps Tweets from Nov 28 to 30th'), \n",
    "                     font=dict(family='Arial',\n",
    "                               size=10),\n",
    "                     yaxis=YAxis(title='Number of Tweets')),\n",
    "}, show_link=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then using Ipython's super [functions](http://blog.fperez.org/2012/09/blogging-with-ipython-notebook.html), I can embed html directly into the IPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"https://plot.ly/~minh5/9.embed\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"https://plot.ly/~minh5/9.embed\"></iframe>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
